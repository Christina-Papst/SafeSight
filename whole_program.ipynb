{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import PIL.Image\n",
    "import cv2\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from transformers import AutoModelForImageClassification, ViTImageProcessor\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification, AutoImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model checks if the image is unsafe for work\n",
    "MODEL = AutoModelForImageClassification.from_pretrained(\"Falconsai/nsfw_image_detection\")\n",
    "PROCESSOR = ViTImageProcessor.from_pretrained('Falconsai/nsfw_image_detection')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chpap\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# this model gives an age range for a person on the picture\n",
    "model = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\n",
    "transforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(image_path):\n",
    "    \n",
    "    # Load the image\n",
    "    img = PIL.Image.open(image_path)\n",
    "    \n",
    "    # Process the image and make predictions if the image is nsfw\n",
    "    with torch.no_grad():\n",
    "        inputs = PROCESSOR(images=img, return_tensors=\"pt\")\n",
    "        outputs = MODEL(**inputs)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Get the predicted label\n",
    "    predicted_label = logits.argmax(-1).item()\n",
    "    \n",
    "    # Map the predicted label to its corresponding class name\n",
    "    label_map = MODEL.config.id2label\n",
    "    predicted_class = label_map[predicted_label]\n",
    "\n",
    "     # this part is classifying the age of the person on the picture\n",
    "    model = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\n",
    "    transforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\n",
    "    inputs = transforms(img, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    proba = outputs.logits.softmax(1)\n",
    "    preds = proba.argmax(1)\n",
    "\n",
    "    id_to_age_range = {\n",
    "        0: \"0-2\",\n",
    "        1: \"3-9\",\n",
    "        2: \"10-19\",\n",
    "        3: \"20-29\",\n",
    "        4: \"30-39\",\n",
    "        5: \"40-49\",\n",
    "        6: \"50-59\",\n",
    "        7: \"60-69\",\n",
    "        8: \"more than 70\"\n",
    "}\n",
    "\n",
    "    # Convert predicted class indices to age ranges\n",
    "    predicted_age_ranges = [id_to_age_range[pred.item()] for pred in preds]\n",
    "\n",
    "    # this part identifies the most prevalent emotion\n",
    "    image_processor = AutoImageProcessor.from_pretrained(\"dima806/facial_emotions_image_detection\")\n",
    "    model = ViTForImageClassification.from_pretrained(\"dima806/facial_emotions_image_detection\")\n",
    "\n",
    "    image = PIL.Image.open(image_path)\n",
    "    image = np.array(image)\n",
    "    image = image[:, :, :3]\n",
    "\n",
    "    inputs = image_processor(image, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    predicted_emotion = logits.argmax(-1).item()\n",
    "\n",
    "    # Check if NSFW and age is less than 20\n",
    "    if predicted_class == \"nsfw\" and any(age_range in [\"0-2\", \"3-9\", \"10-19\"] for age_range in predicted_age_ranges):\n",
    "    # Copy the image to the flagged folder\n",
    "        shutil.copy2(image_path, directory=\"flagged_images\")\n",
    "   \n",
    "    nsfw_negative_emotions = [\"sad\", \"disgust\", \"angry\", \"fear\"]  # List of negative emotions\n",
    "\n",
    "    if predicted_class == \"nsfw\" and any(predicted_emotion == emotion for emotion in nsfw_negative_emotions):\n",
    "    # Copy the image to the flagged folder\n",
    "        shutil.copy2(image_path, flagged_folder_path)\n",
    "    \n",
    "\n",
    "    # show the results in all three categories\n",
    "    print(\"Safe or unsafe for work:\", predicted_class)\n",
    "    print(\"Predicted age ranges:\", predicted_age_ranges)\n",
    "    print(\"The most prevalent emotion is\", model.config.id2label[predicted_emotion])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of fake user agents\n",
    "SCRAPEOPS_API_KEY = 'Enter your Scrapeops API key here'\n",
    "\n",
    "def get_user_agent_list():\n",
    "  response = requests.get('http://headers.scrapeops.io/v1/user-agents?api_key=' + SCRAPEOPS_API_KEY)\n",
    "  json_response = response.json()\n",
    "  return json_response.get('result', [])\n",
    "\n",
    "def get_random_user_agent(user_agent_list):\n",
    "  random_index = randint(0, len(user_agent_list) - 1)\n",
    "  return user_agent_list[random_index]\n",
    "\n",
    "## Retrieve User-Agent List From ScrapeOps\n",
    "user_agent_list = get_user_agent_list()\n",
    "\n",
    "def download_images(url, directory=\"scraped_images\"):\n",
    "  \n",
    "\n",
    "    headers = {'User-Agent': get_random_user_agent(user_agent_list)}  # Assuming you have this function\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    img_tags = soup.find_all('img')\n",
    "\n",
    "    for img in img_tags:\n",
    "        img_url = img['src']\n",
    "\n",
    "        # Handle relative URLs by prepending the base URL\n",
    "        if img_url.startswith('http') or img_url.startswith('https'):\n",
    "            image_url = img_url\n",
    "        else:\n",
    "            image_url = url + img_url\n",
    "\n",
    "        # Generate a unique filename based on the image URL\n",
    "        filename = hashlib.sha256(image_url.encode('utf-8')).hexdigest()[:10] + \".jpg\"\n",
    "\n",
    "        # Save the image\n",
    "        with requests.get(image_url, stream=True) as image_response:\n",
    "            if image_response.status_code == 200:\n",
    "                with open(os.path.join(directory, filename), 'wb') as file:\n",
    "                    for chunk in image_response.iter_content(1024):\n",
    "                        file.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_images(url, directory=\"scraped_images\"):\n",
    "\n",
    "\n",
    "    # Download images\n",
    "    download_images(url, directory)\n",
    "\n",
    "    # Process each image in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        image_path = os.path.join(directory, filename)\n",
    "        predicted_values = classify_image(image_path)  # Call the existing classify_image function\n",
    "  \n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Image: {filename}\")\n",
    "        print(predicted_values)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://depositphotos.com/photos/sexy-fantasy.html?sorting=newest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safe or unsafe for work: normal\n",
      "Predicted age ranges: ['20-29']\n",
      "The most prevalent emotion is neutral\n",
      "Image: 008b015237.jpg\n",
      "None\n",
      "\n",
      "\n",
      "Safe or unsafe for work: normal\n",
      "Predicted age ranges: ['20-29']\n",
      "The most prevalent emotion is neutral\n",
      "Image: 07432ab0b3.jpg\n",
      "None\n",
      "\n",
      "\n",
      "Safe or unsafe for work: normal\n",
      "Predicted age ranges: ['20-29']\n",
      "The most prevalent emotion is surprise\n",
      "Image: 0b5aee546f.jpg\n",
      "None\n",
      "\n",
      "\n",
      "Safe or unsafe for work: normal\n",
      "Predicted age ranges: ['20-29']\n",
      "The most prevalent emotion is sad\n",
      "Image: 10a6fe62fc.jpg\n",
      "None\n",
      "\n",
      "\n",
      "Safe or unsafe for work: normal\n",
      "Predicted age ranges: ['20-29']\n",
      "The most prevalent emotion is neutral\n",
      "Image: 10b20d4194.jpg\n",
      "None\n",
      "\n",
      "\n",
      "Safe or unsafe for work: normal\n",
      "Predicted age ranges: ['20-29']\n",
      "The most prevalent emotion is fear\n",
      "Image: 15a1634d06.jpg\n",
      "None\n",
      "\n",
      "\n",
      "Safe or unsafe for work: normal\n",
      "Predicted age ranges: ['30-39']\n",
      "The most prevalent emotion is angry\n",
      "Image: 16cf443e28.jpg\n",
      "None\n",
      "\n",
      "\n",
      "Safe or unsafe for work: normal\n",
      "Predicted age ranges: ['20-29']\n",
      "The most prevalent emotion is sad\n",
      "Image: 17b7d9b61f.jpg\n",
      "None\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to infer channel dimension format",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrate_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscraped_images\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m, in \u001b[0;36mrate_images\u001b[1;34m(url, directory)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(directory):\n\u001b[0;32m      9\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, filename)\n\u001b[1;32m---> 10\u001b[0m     predicted_values \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Call the existing classify_image function\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Print the results\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m, in \u001b[0;36mclassify_image\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Process the image and make predictions if the image is nsfw\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 8\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mPROCESSOR\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m MODEL(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     10\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[1;32mc:\\Users\\chpap\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\image_processing_utils.py:551\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[1;34m(self, images, **kwargs)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[0;32m    550\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chpap\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\vit\\image_processing_vit.py:264\u001b[0m, in \u001b[0;36mViTImageProcessor.preprocess\u001b[1;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt looks like you are trying to rescale already rescaled images. If the input\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    260\u001b[0m     )\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_data_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;66;03m# We assume that all images have the same channel dimension format.\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     input_data_format \u001b[38;5;241m=\u001b[39m \u001b[43minfer_channel_dimension_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[0;32m    267\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    268\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize(image\u001b[38;5;241m=\u001b[39mimage, size\u001b[38;5;241m=\u001b[39msize_dict, resample\u001b[38;5;241m=\u001b[39mresample, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[0;32m    269\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[0;32m    270\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\chpap\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\image_utils.py:206\u001b[0m, in \u001b[0;36minfer_channel_dimension_format\u001b[1;34m(image, num_channels)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mshape[last_dim] \u001b[38;5;129;01min\u001b[39;00m num_channels:\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChannelDimension\u001b[38;5;241m.\u001b[39mLAST\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to infer channel dimension format\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to infer channel dimension format"
     ]
    }
   ],
   "source": [
    "rate_images(url, directory=\"scraped_images\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
